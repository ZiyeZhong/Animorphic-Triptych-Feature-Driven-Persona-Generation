{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c9ee065967b4ea2b971d3ac3dba4377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46520c05e4ba4240ba4f6090d041104c",
              "IPY_MODEL_e9113dda722a4937848e2a44054f1132",
              "IPY_MODEL_16532583bf5a4f27861dbb6578759caa"
            ],
            "layout": "IPY_MODEL_694f18f1701443469a16d04b2a71567f"
          }
        },
        "46520c05e4ba4240ba4f6090d041104c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9d6ad041c304b8d80f95d774e901262",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2ae55afaec1e4180ab05a4acfe9c5d37",
            "value": "Loadingâ€‡pipelineâ€‡components...:â€‡100%"
          }
        },
        "e9113dda722a4937848e2a44054f1132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d0337a08254e7abced3caa7d1ece0e",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_043bfcaf01114fd3bf090239de841b30",
            "value": 7
          }
        },
        "16532583bf5a4f27861dbb6578759caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1503496d7fbb4c32848889ca67a3cb15",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4af34e47c74647a2909541e2b748c251",
            "value": "â€‡7/7â€‡[00:04&lt;00:00,â€‡â€‡1.45it/s]"
          }
        },
        "694f18f1701443469a16d04b2a71567f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9d6ad041c304b8d80f95d774e901262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ae55afaec1e4180ab05a4acfe9c5d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76d0337a08254e7abced3caa7d1ece0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "043bfcaf01114fd3bf090239de841b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1503496d7fbb4c32848889ca67a3cb15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4af34e47c74647a2909541e2b748c251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import the drive module from the google.colab library to interact with your Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive to the Colab virtual machine at the directory '/content/drive'\n",
        "# This requires you to approve permission in a pop-up window\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change the current working directory (CWD) to your specific project folder using the magic command %cd\n",
        "# %cd ensures the directory change persists for all subsequent cells (unlike !cd which only affects one line)\n",
        "%cd /content/drive/MyDrive/CIS5810 Fa25/project_final\n",
        "\n",
        "# Optional: Verify the current directory and list files to make sure you are in the right place\n",
        "import os\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seTjHbASbfV1",
        "outputId": "7dc9bda9-077c-4a70-9c41-5a33f6c5196d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/CIS5810 Fa25/project_final\n",
            "Current Working Directory: /content/drive/MyDrive/CIS5810 Fa25/project_final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INSTALLATION COMMANDS (Run these once to set up the Colab environment) ---\n",
        "\n",
        "# Install OpenCV (headless version for server environments without a display), Pillow for image manipulation, and MediaPipe for facial landmarks\n",
        "!pip install -q opencv-python-headless pillow mediapipe\n",
        "\n",
        "# Install Hugging Face libraries: 'diffusers' for Stable Diffusion, 'transformers' for pre-trained models, 'accelerate' for GPU optimization, and 'safetensors' for secure model loading\n",
        "!pip install -q diffusers transformers accelerate safetensors\n",
        "\n",
        "# Install the official OpenAI client library to interact with GPT-4 Vision API\n",
        "!pip install -q openai\n",
        "\n",
        "# Install Streamlit for building the web interface and pyngrok to expose the local Colab server to the public internet\n",
        "!pip install -q streamlit pyngrok\n",
        "\n",
        "# --- IMPORT STATEMENTS (Bringing necessary tools into the script) ---\n",
        "\n",
        "# Import OpenCV library for computer vision tasks (specifically for Canny edge detection and image array handling)\n",
        "import cv2\n",
        "\n",
        "# Import NumPy for numerical operations, essential because images are processed as multi-dimensional arrays (matrices)\n",
        "import numpy as np\n",
        "\n",
        "# Import PyTorch, the deep learning framework that powers the Stable Diffusion generation pipeline\n",
        "import torch\n",
        "\n",
        "# Import the Image module from Pillow (PIL) to handle image loading, resizing, and saving in a user-friendly format\n",
        "from PIL import Image\n",
        "\n",
        "# Import MediaPipe solutions to access the pre-trained Face Mesh model for detecting 478 facial landmarks\n",
        "import mediapipe as mp\n",
        "\n",
        "# Import Pyplot from Matplotlib to display images directly within the Google Colab notebook cells for debugging\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import BytesIO to handle binary streams, allowing us to keep images in memory without saving to disk constantly\n",
        "from io import BytesIO\n",
        "\n",
        "# Import Base64 to encode image binary data into text strings, which is required to send images to the OpenAI API\n",
        "import base64\n",
        "\n",
        "# Import JSON library to parse the structured text output (dictionaries) returned by GPT-4\n",
        "import json\n",
        "\n",
        "# Import OS module to interact with the file system, check if files exist, and handle API keys securely\n",
        "import os"
      ],
      "metadata": {
        "id": "n6D5ysTGX14B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SYSTEM CHECK ---\n",
        "\n",
        "# Check if a GPU (Graphics Processing Unit) is available to PyTorch; this is critical for the generation speed of Stable Diffusion\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "-eyCSATTBf4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854a3909-605f-4220-d6bc-352eebcd903e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the userdata module from Google Colab to securely access stored secrets (API keys)\n",
        "from google.colab import userdata\n",
        "\n",
        "# Import the OpenAI class to create the interface for interacting with GPT models\n",
        "from openai import OpenAI\n",
        "\n",
        "# Import the standard 'sys' module to stop execution if necessary (optional, but good for safety)\n",
        "import sys\n",
        "\n",
        "# --- STEP 1: RETRIEVE API KEY ---\n",
        "\n",
        "# Initialize a variable to store the key\n",
        "api_key = None\n",
        "\n",
        "try:\n",
        "    # Attempt to retrieve the 'OPENAI_API_KEY' from Colab's secure \"Secrets\" store\n",
        "    # This is the most secure method as the key is not visible in the code\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"Success: API Key found in Colab Secrets.\")\n",
        "\n",
        "except (userdata.SecretNotFoundError, userdata.NotebookAccessError):\n",
        "    # If the secret doesn't exist or permissions are missing, catch the specific error\n",
        "    print(\"Notice: 'OPENAI_API_KEY' not found in Secrets. Switching to manual input.\")\n",
        "    pass # Continue to the manual input block below\n",
        "\n",
        "# --- STEP 2: MANUAL FALLBACK & VALIDATION ---\n",
        "\n",
        "# If the key is still None (because Secrets failed) or empty, ask the user manually\n",
        "if not api_key:\n",
        "    # Prompt the user to paste the key; usage of .strip() removes accidental leading/trailing spaces\n",
        "    api_key = input(\"Please enter your OpenAI API Key: \").strip()\n",
        "\n",
        "# --- STEP 3: FINAL VALIDATION ---\n",
        "\n",
        "# specific check: ensure the user didn't just press \"Enter\" leaving the key empty\n",
        "if not api_key:\n",
        "    # Print a critical error message in red (using ANSI escape codes) for visibility\n",
        "    print(\"\\033[91mError: No API Key provided. The script cannot proceed.\\033[0m\")\n",
        "    # Stop the script execution immediately to prevent crashing later\n",
        "    sys.exit(\"Script stopped due to missing API Key.\")\n",
        "\n",
        "# --- STEP 4: INITIALIZE CLIENT ---\n",
        "\n",
        "# Initialize the OpenAI client with the validated key\n",
        "# This client object will be used for all subsequent calls (GPT-4, DALL-E, etc.)\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Confirm successful configuration to the user\n",
        "print(\"OpenAI Client successfully configured and ready.\")"
      ],
      "metadata": {
        "id": "5lFUUuxwDgEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bbf337-d7e9-4a4d-c098-518feda53ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: API Key found in Colab Secrets.\n",
            "OpenAI Client successfully configured and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1 + 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dfdtrbOqxWm",
        "outputId": "637dd313-9074-4bf5-84f7-8cddcfefb560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MediaPipe Face Mesh globally to prevent reloading the heavy model every time the function is called\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "def get_face_landmarks(image_path):\n",
        "    \"\"\"\n",
        "    Detects face landmarks using MediaPipe and loads the image once.\n",
        "    \"\"\"\n",
        "    # Load the image from the disk using OpenCV (loads in BGR format)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Validation check: if the image path is wrong or file is corrupted, 'image' will be None\n",
        "    if image is None:\n",
        "        # Raise an error to stop execution and inform the user\n",
        "        raise FileNotFoundError(f\"Image not found at: {image_path}\")\n",
        "\n",
        "    # Convert the image from BGR (OpenCV standard) to RGB (MediaPipe requirement)\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Open the FaceMesh model with specific settings using a context manager ('with') to handle memory automatically\n",
        "    with mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=True,       # We are processing a single photo, not a video stream\n",
        "        max_num_faces=1,              # Limit detection to the most prominent face only\n",
        "        refine_landmarks=True,        # Enable detailed attention to eyes and lips (478 points)\n",
        "        min_detection_confidence=0.5  # Set confidence threshold to 50%\n",
        "    ) as face_mesh:\n",
        "\n",
        "        # Run the inference to detect facial landmarks on the RGB image\n",
        "        results = face_mesh.process(rgb_image)\n",
        "\n",
        "    # Check if any faces were actually detected\n",
        "    if not results.multi_face_landmarks:\n",
        "        # If the list is empty, raise an error\n",
        "        raise ValueError(\"No face detected. Please try a clearer photo.\")\n",
        "\n",
        "    # Return the landmarks of the first face found [0], and the original BGR image array for later use\n",
        "    return results.multi_face_landmarks[0], image\n",
        "\n",
        "\n",
        "def create_feature_canny_map(image_bgr, feature_name, landmarks):\n",
        "    \"\"\"\n",
        "    Generates a Canny edge map isolated to a specific facial feature.\n",
        "    Handles 'split' features (like left/right eyes) separately to avoid incorrect masking.\n",
        "    \"\"\"\n",
        "    # Extract height (h) and width (w) from the image array shape\n",
        "    h, w, _ = image_bgr.shape\n",
        "\n",
        "    # --- 1. FEATURE DEFINITIONS ---\n",
        "    # We use a LIST OF LISTS structure.\n",
        "    # Single regions (like mouth) have one inner list: [[points]].\n",
        "    # Split regions (like eyes) have two inner lists: [[left_points], [right_points]].\n",
        "\n",
        "    feature_indices = {\n",
        "        # EYES: Split into Left and Right so the mask doesn't cover the nose bridge\n",
        "        \"eyes\": [\n",
        "            [33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7], # Left Eye Loop\n",
        "            [362, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382] # Right Eye Loop\n",
        "        ],\n",
        "\n",
        "        # MOUTH: Single region containing lips and mouth opening\n",
        "        \"mouth\": [[61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 324, 318, 402, 317,\n",
        "                   14, 87, 178, 88, 95, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]],\n",
        "\n",
        "        # JAWLINE: Single region describing the lower face curve\n",
        "        \"jawline\": [[234, 93, 132, 58, 172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379,\n",
        "                     365, 397, 288, 361, 323, 454, 356]],\n",
        "\n",
        "        # NOSE: Single region for nose tip and nostrils\n",
        "        \"nose\": [[1, 2, 98, 327, 168, 6, 197, 195, 5, 4, 45, 275, 440, 220, 243]],\n",
        "\n",
        "        # EYEBROWS: Split Left and Right so we don't draw a \"unibrow\" mask across the forehead\n",
        "        \"eyebrows\": [\n",
        "            [70, 63, 105, 66, 107, 55, 65, 52, 53, 46],  # Left Brow\n",
        "            [336, 296, 334, 293, 300, 276, 283, 282, 295, 285] # Right Brow\n",
        "        ],\n",
        "\n",
        "        # CHEEKS: Split Left and Right so we don't mask out the nose/mouth in the center\n",
        "        \"cheeks\": [\n",
        "            [116, 123, 147, 213, 192, 214], # Left Cheek area\n",
        "            [345, 352, 376, 433, 416, 434]  # Right Cheek area\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # --- 2. GLOBAL CANNY GENERATION ---\n",
        "\n",
        "    # Convert the BGR image to Grayscale (0-255) for edge detection\n",
        "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate the median pixel intensity of the image\n",
        "    v = np.median(gray)\n",
        "\n",
        "    # Set a tightness parameter (sigma) for the dynamic thresholding\n",
        "    sigma = 0.33\n",
        "\n",
        "    # Calculate lower threshold (max ensures it's not negative)\n",
        "    lower = int(max(0, (1.0 - sigma) * v))\n",
        "\n",
        "    # Calculate upper threshold (min ensures it doesn't exceed 255)\n",
        "    upper = int(min(255, (1.0 + sigma) * v))\n",
        "\n",
        "    # Apply Canny algorithm to find all edges in the image using calculated thresholds\n",
        "    canny_full = cv2.Canny(gray, lower, upper)\n",
        "\n",
        "    # --- 3. MASK GENERATION ---\n",
        "\n",
        "    # Create a black canvas (all zeros) with the same size as the grayscale image\n",
        "    mask = np.zeros_like(gray)\n",
        "\n",
        "    # Normalize the input feature name to lowercase for dictionary lookup\n",
        "    fname = feature_name.lower()\n",
        "\n",
        "    # Check if the requested feature exists in our definitions\n",
        "    if fname not in feature_indices:\n",
        "        print(f\"Warning: {fname} not found. Returning full edges.\")\n",
        "        # If unknown, return the full edge map without masking\n",
        "        return Image.fromarray(canny_full)\n",
        "\n",
        "    # Retrieve the list of regions for the requested feature (e.g., [[left_pts], [right_pts]])\n",
        "    regions = feature_indices[fname]\n",
        "\n",
        "    # Loop through each sub-region separately (CRITICAL FIX for split features)\n",
        "    for indices in regions:\n",
        "        points = []\n",
        "\n",
        "        # Convert landmark indices to pixel coordinates\n",
        "        for idx in indices:\n",
        "            pt = landmarks.landmark[idx] # Get normalized point (0.0-1.0)\n",
        "            x = int(pt.x * w)            # Scale to image width\n",
        "            y = int(pt.y * h)            # Scale to image height\n",
        "            points.append((x, y))        # Add to list\n",
        "\n",
        "        # Convert list of tuples to a NumPy array for OpenCV drawing functions\n",
        "        points_np = np.array(points, dtype=np.int32)\n",
        "\n",
        "        # Draw the white region on the black mask\n",
        "        if fname == \"jawline\":\n",
        "             # For jawline, we draw a thick open line (polyline) because it's not a loop\n",
        "             cv2.polylines(mask, [points_np], isClosed=False, color=255, thickness=40)\n",
        "        else:\n",
        "             # For all other features (eyes, nose, etc.), we draw a filled closed polygon\n",
        "             # convexHull wraps the points in the smallest possible \"rubber band\" shape\n",
        "             hull = cv2.convexHull(points_np)\n",
        "\n",
        "             # Fill the hull with white (255)\n",
        "             cv2.fillConvexPoly(mask, hull, 255)\n",
        "\n",
        "    # --- 4. MASK REFINEMENT ---\n",
        "\n",
        "    # Create a 15x15 kernel of 1s for dilation\n",
        "    kernel = np.ones((15,15), np.uint8)\n",
        "\n",
        "    # Dilate (expand) the white mask slightly to include edges bordering the feature\n",
        "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
        "\n",
        "    # --- 5. FINAL COMBINATION ---\n",
        "\n",
        "    # Perform Bitwise AND: Only keep pixels from 'canny_full' where 'mask' is white\n",
        "    feature_canny = cv2.bitwise_and(canny_full, canny_full, mask=mask)\n",
        "\n",
        "    # Convert the resulting NumPy array to a PIL Image object (required for the Diffusion pipeline)\n",
        "    return Image.fromarray(feature_canny)"
      ],
      "metadata": {
        "id": "DnCbbZoak_CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_face_with_gpt4(image_path):\n",
        "    \"\"\"\n",
        "    Sends image to GPT-4 Vision to select 3 animals from a FIXED POOL based on facial features.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. IMAGE ENCODING ---\n",
        "    # Open the image file in binary read mode\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        # Encode the image to base64 string so it can be sent via JSON API\n",
        "        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "    # --- 2. PROMPT ENGINEERING WITH FIXED POOL ---\n",
        "\n",
        "    prompt_text = \"\"\"\n",
        "    You are an expert visual-anthropology model.\n",
        "    Analyze the provided human portrait.\n",
        "\n",
        "    TASK:\n",
        "    Select EXACTLY THREE animals from the fixed pool below that best resemble the person's visual traits.\n",
        "\n",
        "    === ALLOWED ANIMAL POOL (Choose 3) ===\n",
        "    [\"Bear\", \"Beaver\", \"Cat\", \"Deer\", \"Eagle\", \"Elephant\", \"Fox\", \"Hamster\",\n",
        "     \"Horse\", \"Koala\", \"Lamb\", \"Lion\", \"Meerkat\", \"Owl\", \"Panda\", \"Puppy\",\n",
        "     \"Racoon\", \"Sloth\", \"Tiger\", \"Wolf\"]\n",
        "\n",
        "    === ALLOWED FEATURE POOL (Choose 3 distinct) ===\n",
        "    [\"eyes\", \"jawline\", \"mouth\", \"nose\", \"eyebrows\", \"cheeks\"]\n",
        "\n",
        "    === RULES ===\n",
        "    1. STRICT RESTRICTION: Do NOT use any animal outside the Allowed Pool.\n",
        "    2. STRICT RESTRICTION: The \"feature\" must be exactly one of the Allowed Features.\n",
        "    3. DIVERSITY: Select 3 DIFFERENT animals and 3 DIFFERENT features.\n",
        "       (e.g. Do not use 'eyes' for all three animals).\n",
        "    4. OUTPUT FORMAT: Return ONLY raw JSON.\n",
        "\n",
        "    === JSON EXAMPLE ===\n",
        "    {\n",
        "      \"animals\": [\n",
        "        {\"animal\": \"Owl\", \"trait\": \"wise, observant\", \"feature\": \"eyes\"},\n",
        "        {\"animal\": \"Fox\", \"trait\": \"clever, alert\", \"feature\": \"nose\"},\n",
        "        {\"animal\": \"Deer\", \"trait\": \"gentle, soft\", \"feature\": \"jawline\"}\n",
        "      ]\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 3. API CALL ---\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\", # Recommended model for best instruction following\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": prompt_text},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "            response_format={\"type\": \"json_object\"} # Enforce JSON structure\n",
        "        )\n",
        "\n",
        "        # --- 4. PARSING RESPONSE ---\n",
        "        content = response.choices[0].message.content\n",
        "        result_json = json.loads(content)\n",
        "\n",
        "        # Validation: Check if returned animals are actually in our pool (Optional safety check)\n",
        "        valid_pool = [\"bear\", \"beaver\", \"cat\", \"deer\", \"eagle\", \"elephant\", \"fox\", \"hamster\",\n",
        "                      \"horse\", \"koala\", \"lamb\", \"lion\", \"meerkat\", \"owl\", \"panda\", \"puppy\",\n",
        "                      \"racoon\", \"sloth\", \"tiger\", \"wolf\"]\n",
        "\n",
        "        for item in result_json.get(\"animals\", []):\n",
        "            if item['animal'].lower() not in valid_pool:\n",
        "                print(f\"Warning: GPT returned {item['animal']} which is not in pool. Replacing with 'Cat'.\")\n",
        "                item['animal'] = \"Cat\" # Fallback mechanism\n",
        "\n",
        "        return result_json\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPT-4 analysis: {e}\")\n",
        "        # Return a safe fallback from the pool\n",
        "        return {\n",
        "            \"animals\": [\n",
        "                {\"animal\": \"Cat\", \"trait\": \"default\", \"feature\": \"eyes\"},\n",
        "                {\"animal\": \"Puppy\", \"trait\": \"default\", \"feature\": \"mouth\"},\n",
        "                {\"animal\": \"Eagle\", \"trait\": \"default\", \"feature\": \"nose\"}\n",
        "            ]\n",
        "        }"
      ],
      "metadata": {
        "id": "yiR-cYYsDrae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, DPMSolverMultistepScheduler\n",
        "\n",
        "def load_generation_pipeline():\n",
        "    \"\"\"\n",
        "    Loads the SDXL ControlNet pipeline with memory optimizations for Colab T4 GPU.\n",
        "    \"\"\"\n",
        "    print(\"Loading ControlNet Model...\")\n",
        "    # 1. Load ControlNet for Canny Edge (SDXL version)\n",
        "    # This model translates the white-on-black edge maps into guidance for the AI\n",
        "    controlnet = ControlNetModel.from_pretrained(\n",
        "        \"diffusers/controlnet-canny-sdxl-1.0\",\n",
        "        torch_dtype=torch.float16,  # Use half-precision to save VRAM\n",
        "        use_safetensors=True        # Faster and safer model loading format\n",
        "    )\n",
        "\n",
        "    print(\"Loading VAE...\")\n",
        "    # 2. Load VAE (Variational Autoencoder)\n",
        "    # We use a specific \"fp16-fix\" version. Standard SDXL VAE often produces black images or \"NaN\" errors\n",
        "    # when running in float16 precision. This fixed version prevents that.\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        \"madebyollin/sdxl-vae-fp16-fix\",\n",
        "        torch_dtype=torch.float16,\n",
        "        use_safetensors=True\n",
        "    )\n",
        "\n",
        "    print(\"Loading Main SDXL Pipeline...\")\n",
        "    # 3. Load the Main Pipeline (Stable Diffusion XL Base 1.0)\n",
        "    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        controlnet=controlnet,\n",
        "        vae=vae,\n",
        "        torch_dtype=torch.float16,\n",
        "        use_safetensors=True,\n",
        "        variant=\"fp16\" # Loads the smaller fp16 weights directly to save download time\n",
        "    )\n",
        "\n",
        "    # 4. OPTIMIZATION: Scheduler Upgrade\n",
        "    # Switch to DPM++ 2M Karras. It produces better details in fewer steps (25-30 steps)\n",
        "    # compared to the default scheduler (which often needs 50+).\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n",
        "        pipe.scheduler.config,\n",
        "        use_karras_sigmas=True,\n",
        "        algorithm_type=\"dpmsolver++\"\n",
        "    )\n",
        "\n",
        "    # 5. OPTIMIZATION: Memory Management for Colab T4 (16GB VRAM)\n",
        "    # \"cpu_offload\" keeps only the active component (e.g., UNet) in GPU VRAM,\n",
        "    # and moves others (Text Encoder, VAE) to CPU RAM when not in use.\n",
        "    # This is CRITICAL to avoid \"RuntimeError: CUDA out of memory\".\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    # Optional: Enable VAE slicing to save even more memory during the final image decoding phase\n",
        "    pipe.enable_vae_slicing()\n",
        "\n",
        "    # VAE Tiling: Decodes in tiles (The most important fix for 1024x1024 on T4)\n",
        "    pipe.enable_vae_tiling()\n",
        "\n",
        "    return pipe\n",
        "\n",
        "# Initialize pipeline (Run this once, it takes about 1-2 minutes to download ~6GB of models)\n",
        "pipe = load_generation_pipeline()\n",
        "print(\"âœ… Stable Diffusion XL ControlNet Pipeline Loaded Successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "6c9ee065967b4ea2b971d3ac3dba4377",
            "46520c05e4ba4240ba4f6090d041104c",
            "e9113dda722a4937848e2a44054f1132",
            "16532583bf5a4f27861dbb6578759caa",
            "694f18f1701443469a16d04b2a71567f",
            "a9d6ad041c304b8d80f95d774e901262",
            "2ae55afaec1e4180ab05a4acfe9c5d37",
            "76d0337a08254e7abced3caa7d1ece0e",
            "043bfcaf01114fd3bf090239de841b30",
            "1503496d7fbb4c32848889ca67a3cb15",
            "4af34e47c74647a2909541e2b748c251"
          ]
        },
        "id": "WL_ik3IRDxPF",
        "outputId": "8bd5be71-e13c-4924-be7b-2e623dac7ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ControlNet Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading VAE...\n",
            "Loading Main SDXL Pipeline...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c9ee065967b4ea2b971d3ac3dba4377"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Stable Diffusion XL ControlNet Pipeline Loaded Successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc # Garbage Collector interface\n",
        "\n",
        "def process_animorphic_triptych(image_path):\n",
        "    print(f\"\\nðŸš€ Starting processing for: {image_path}\")\n",
        "\n",
        "    # --- PHASE 1: ANALYSIS ---\n",
        "    print(\"Step 1/4: Extracting Face Landmarks...\")\n",
        "    landmarks, image_bgr = get_face_landmarks(image_path)\n",
        "\n",
        "    print(\"Step 2/4: Consulting GPT-4 Visual Anthropologist...\")\n",
        "    analysis = analyze_face_with_gpt4(image_path)\n",
        "\n",
        "    if \"animals\" not in analysis:\n",
        "        print(\"Error: GPT response invalid.\")\n",
        "        return\n",
        "\n",
        "    animals_data = analysis[\"animals\"]\n",
        "    print(f\"GPT Output: {json.dumps(animals_data, indent=2)}\")\n",
        "\n",
        "    # --- PHASE 2: SYNTHESIS LOOP ---\n",
        "    generated_results = []\n",
        "    print(f\"Step 3/4: Generating 3 Animal Portraits...\")\n",
        "\n",
        "    for i, item in enumerate(animals_data):\n",
        "        animal = item['animal']\n",
        "        trait = item['trait']\n",
        "        feature = item['feature']\n",
        "\n",
        "        print(f\"  > Processing {i+1}/3: {animal} (focus: {feature})...\")\n",
        "\n",
        "        # 1. Prepare Canny Map\n",
        "        canny_map = create_feature_canny_map(image_bgr, feature, landmarks)\n",
        "\n",
        "        # 2. Prompt\n",
        "        prompt = (\n",
        "            f\"A majestic {trait} {animal} portrait, close-up, highly detailed, \"\n",
        "            f\"cinematic lighting, 8k resolution, photorealistic texture, \"\n",
        "            f\"expressive {feature}, artstation trending, masterpiece.\"\n",
        "        )\n",
        "        negative_prompt = \"human face, skin, cartoon, anime, sketch, blurry, low quality, distorted, deformed, text, watermark\"\n",
        "\n",
        "        # 3. Generate Image\n",
        "        try:\n",
        "            image = pipe(\n",
        "                prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                image=canny_map,\n",
        "                controlnet_conditioning_scale=0.65,\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5\n",
        "            ).images[0]\n",
        "\n",
        "            generated_results.append({\n",
        "                \"animal\": animal,\n",
        "                \"feature\": feature,\n",
        "                \"trait\": trait,\n",
        "                \"canny\": canny_map,\n",
        "                \"image\": image\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating {animal}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # --- CRITICAL FIX: FORCE MEMORY CLEANUP ---\n",
        "        # After each generation, force clear the GPU cache to make room for the next one\n",
        "        print(\"    ...Cleaning GPU memory...\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # --- PHASE 3: VISUALIZATION ---\n",
        "    print(\"Step 4/4: Visualization...\")\n",
        "\n",
        "    if not generated_results:\n",
        "        print(\"No images were generated successfully.\")\n",
        "        return\n",
        "\n",
        "    # Dynamic figure size based on how many images succeeded\n",
        "    count = len(generated_results) + 1\n",
        "    fig, axes = plt.subplots(1, count, figsize=(5 * count, 6))\n",
        "\n",
        "    # Handle single axis case if only 1 image generated\n",
        "    if count == 1: axes = [axes]\n",
        "\n",
        "    # Show Original\n",
        "    axes[0].imshow(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Show Generated\n",
        "    for i, res in enumerate(generated_results):\n",
        "        axes[i+1].imshow(res['image'])\n",
        "        axes[i+1].set_title(f\"{res['animal']}\\n({res['feature']})\")\n",
        "        axes[i+1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return generated_results\n",
        "\n",
        "# --- RUN AGAIN ---\n",
        "input_image = \"test_face.jpg\"\n",
        "if os.path.exists(input_image):\n",
        "    results = process_animorphic_triptych(input_image)\n",
        "else:\n",
        "    print(f\"Please check if {input_image} exists.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "V38bKFsymYS7",
        "outputId": "79cadb71-34ae-4c17-fbbe-7fb7411d8d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-453469968.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# --- RUN AGAIN ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_face.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_animorphic_triptych\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_animorphic_triptych(image_path):\n",
        "    # 1. Resize/Preprocess Image (Ensure it's 1024x1024 for SDXL best results)\n",
        "    original_img = Image.open(image_path).convert(\"RGB\")\n",
        "    original_img = original_img.resize((1024, 1024))\n",
        "    original_img.save(\"aligned_input.jpg\")\n",
        "\n",
        "    # 2. Get Landmarks\n",
        "    print(\"Extracting landmarks...\")\n",
        "    landmarks, shape = get_face_landmarks(\"aligned_input.jpg\")\n",
        "\n",
        "    # 3. Get GPT-4 Analysis\n",
        "    print(\"Analyzing face with GPT-4...\")\n",
        "    analysis = analyze_face_with_gpt4(\"aligned_input.jpg\")\n",
        "    print(\"GPT Analysis:\", json.dumps(analysis, indent=2))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # 4. Loop through the 3 archetypes\n",
        "    for item in analysis['animals']:\n",
        "        animal = item['animal']\n",
        "        trait = item['trait']\n",
        "        feature = item['feature']\n",
        "\n",
        "        print(f\"Generating {animal} based on {feature}...\")\n",
        "\n",
        "        # A. Create the Feature-Specific Canny Map\n",
        "        canny_image = create_feature_canny_map(\"aligned_input.jpg\", feature, landmarks, shape)\n",
        "\n",
        "        # B. Define the Prompt\n",
        "        # We use a structured prompt for SDXL\n",
        "        prompt = f\"A highly detailed, artistic portrait of a {animal}, {trait}, cinematic lighting, 8k resolution, photorealistic, intricate textures.\"\n",
        "        negative_prompt = \"cartoon, drawing, anime, blurry, low quality, distorted, human face\"\n",
        "\n",
        "        # C. Generate Image\n",
        "        # controlnet_conditioning_scale determines how strictly it follows the lines\n",
        "        # For eyes/mouth, we want strict adherence (0.7-0.9). For jawline, maybe less (0.5-0.7).\n",
        "        scale = 0.8 if feature != \"jawline\" else 0.6\n",
        "\n",
        "        gen_image = pipe(\n",
        "            prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            image=canny_image,\n",
        "            controlnet_conditioning_scale=scale,\n",
        "            num_inference_steps=30,\n",
        "            guidance_scale=7.5\n",
        "        ).images[0]\n",
        "\n",
        "        results.append({\n",
        "            \"animal\": animal,\n",
        "            \"feature\": feature,\n",
        "            \"canny_map\": canny_image,\n",
        "            \"result\": gen_image\n",
        "        })\n",
        "\n",
        "    return original_img, results\n",
        "\n",
        "# --- RUNNING THE TEST ---\n",
        "# Upload an image to Colab first and name it 'test_face.jpg'\n",
        "# OR uncomment below to upload interactively:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# image_path = list(uploaded.keys())[0]\n",
        "\n",
        "# For demo, assuming you have 'test_face.jpg'\n",
        "if os.path.exists('test_face.jpg'):\n",
        "    original, output_data = process_animorphic_triptych('test_face.jpg')\n",
        "\n",
        "    # Display using Matplotlib for immediate check\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Show Original\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.imshow(original)\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Show Generations\n",
        "    for i, data in enumerate(output_data):\n",
        "        plt.subplot(1, 4, i+2)\n",
        "        plt.imshow(data['result'])\n",
        "        plt.title(f\"{data['animal']}\\n({data['feature']})\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Please upload an image named 'test_face.jpg' to run the test.\")"
      ],
      "metadata": {
        "id": "9uPsr8U0EIHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import mediapipe as mp\n",
        "import base64\n",
        "import json\n",
        "import torch\n",
        "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL\n",
        "\n",
        "# --- PAGE CONFIG ---\n",
        "st.set_page_config(page_title=\"Animorphic Triptych\", layout=\"wide\")\n",
        "\n",
        "st.title(\"ðŸ¦ Animorphic Triptych\")\n",
        "st.markdown(\"### From Human Portrait to Animal Archetypes\")\n",
        "\n",
        "# --- SETUP (Copy core functions here or import them if structured as a module) ---\n",
        "# For simplicity in this single-file demo, we assume the pipeline is loaded\n",
        "# inside the app or cached. Due to Colab RAM limits, reloading heavy models\n",
        "# inside Streamlit subprocess might crash if not handled carefully.\n",
        "# IN COLAB: It is better to use the notebook for processing and Streamlit just for display\n",
        "# But for a standalone app, you would load models here using @st.cache_resource\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    # Only load if not already in memory to save time\n",
        "    controlnet = ControlNetModel.from_pretrained(\n",
        "        \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16\n",
        "    )\n",
        "    vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        controlnet=controlnet, vae=vae, torch_dtype=torch.float16\n",
        "    )\n",
        "    pipe.enable_model_cpu_offload()\n",
        "    return pipe\n",
        "\n",
        "# Placeholder for the processing logic (users should copy the functions from Step 3 & 4 here)\n",
        "# ... [Insert get_face_landmarks, create_feature_canny_map, analyze_face_with_gpt4 here] ...\n",
        "\n",
        "# --- UI LOGIC ---\n",
        "uploaded_file = st.file_uploader(\"Upload a Portrait\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Display Original\n",
        "    image = Image.open(uploaded_file).convert(\"RGB\")\n",
        "    st.image(image, caption=\"Original Portrait\", width=300)\n",
        "\n",
        "    if st.button(\"Generate Triptych\"):\n",
        "        with st.spinner(\"Loading AI Models...\"):\n",
        "            pipe = load_models()\n",
        "\n",
        "        # Save temp file for OpenCV\n",
        "        image.save(\"temp_input.jpg\")\n",
        "\n",
        "        # 1. GPT Analysis\n",
        "        with st.spinner(\"Analyzing Personality & Features...\"):\n",
        "            # Dummy mock for UI testing if API key not set, else real call\n",
        "            # analysis = analyze_face_with_gpt4(\"temp_input.jpg\")\n",
        "            st.warning(\"Please ensure API Key and functions are fully integrated in app.py source.\")\n",
        "\n",
        "        # ... logic to generate and display columns ...\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "        with col1:\n",
        "            st.markdown(\"#### Animal 1\")\n",
        "            # st.image(result1)\n",
        "\n",
        "        with col2:\n",
        "            st.markdown(\"#### Animal 2\")\n",
        "\n",
        "        with col3:\n",
        "            st.markdown(\"#### Animal 3\")\n",
        "\n",
        "st.info(\"Note: This is a skeletal UI. To run fully in Colab, copy all helper functions into app.py\")"
      ],
      "metadata": {
        "id": "GTp9DNpCEMcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Streamlit in the background\n",
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "# Expose via LocalTunnel\n",
        "import urllib\n",
        "print(\"Password/IP for LocalTunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "RrOy7P7wEQPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}